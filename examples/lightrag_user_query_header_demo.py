"""
ç¤ºä¾‹ï¼šä½¿ç”¨ä¸åŒçš„æ¨¡å‹å¤„ç†ç”¨æˆ·æŸ¥è¯¢å’ŒçŸ¥è¯†åº“æ„å»º

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ç”¨æˆ·æŸ¥è¯¢å’Œç³»ç»Ÿå†…éƒ¨è°ƒç”¨æ—¶ä½¿ç”¨ä¸åŒçš„ LLM æ¨¡å‹ã€‚
- ç”¨æˆ·æŸ¥è¯¢ï¼ˆAPI è°ƒç”¨ï¼‰ï¼šä½¿ç”¨ LLM_QUERY_MODELï¼ˆä¾‹å¦‚ï¼šäº‘ç«¯å¤§æ¨¡å‹ gpt-4oï¼‰
- å†…éƒ¨è°ƒç”¨ï¼ˆçŸ¥è¯†åº“æ„å»ºï¼‰ï¼šä½¿ç”¨ LLM_MODELï¼ˆä¾‹å¦‚ï¼šæœ¬åœ°æ¨¡å‹ qwen-localï¼‰

ä½¿ç”¨åœºæ™¯ï¼š
1. ç›¸åŒçš„ API æœåŠ¡åœ°å€ï¼Œé€šè¿‡ä¸åŒçš„æ¨¡å‹åç§°è·¯ç”±åˆ°ä¸åŒçš„æ¨¡å‹å®ä¾‹
2. æˆæœ¬ä¼˜åŒ–ï¼šç”¨æˆ·æŸ¥è¯¢ä½¿ç”¨é«˜æ€§èƒ½äº‘ç«¯æ¨¡å‹ï¼Œå†…éƒ¨ä»»åŠ¡ä½¿ç”¨ç»æµçš„æœ¬åœ°æ¨¡å‹
3. æ€§èƒ½ä¼˜åŒ–ï¼šç”¨æˆ·æŸ¥è¯¢ä½¿ç”¨å¿«é€Ÿå“åº”çš„æ¨¡å‹ï¼Œåå°ä»»åŠ¡ä½¿ç”¨æ‰¹å¤„ç†ä¼˜åŒ–çš„æ¨¡å‹
4. åç«¯æœåŠ¡å™¨æ ¹æ®æ¨¡å‹åç§°è·¯ç”±åˆ°ä¸åŒçš„å®ä¾‹ï¼ˆå¦‚é˜¿é‡Œäº‘ Qwen vs æœ¬åœ° Qwenï¼‰

è¿è¡Œè¦æ±‚ï¼š
- éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆé€šè¿‡ .env æ–‡ä»¶æˆ–ç›´æ¥è®¾ç½®ï¼‰
- OPENAI_API_KEY æˆ– LLM_BINDING_API_KEYï¼šLLM API å¯†é’¥
- LLM_BINDING_HOSTï¼šLLM æœåŠ¡åœ°å€ï¼ˆé»˜è®¤ï¼šhttps://api.openai.com/v1ï¼‰
- LLM_MODELï¼šé»˜è®¤ LLM æ¨¡å‹åç§°ï¼ˆç”¨äºçŸ¥è¯†åº“æ„å»ºï¼Œé»˜è®¤ï¼šgpt-4o-miniï¼‰
- LLM_QUERY_MODELï¼šç”¨æˆ·æŸ¥è¯¢ä¸“ç”¨æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸è®¾ç½®åˆ™ä½¿ç”¨ LLM_MODELï¼‰
- EMBEDDING_BINDING_HOSTï¼šEmbedding æœåŠ¡åœ°å€ï¼ˆé»˜è®¤ï¼šhttp://localhost:11434ï¼‰
- EMBEDDING_MODELï¼šEmbedding æ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼šbge-m3:latestï¼‰
"""

import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import openai_complete_if_cache
from lightrag.llm.ollama import ollama_embed
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status
from dotenv import load_dotenv

load_dotenv(dotenv_path=".env", override=False)

WORKING_DIR = "./demoDir"


async def llm_model_func(
    prompt, 
    system_prompt=None, 
    history_messages=[], 
    keyword_extraction=False,
    llm_query_model=None,  # ç”¨æˆ·æŸ¥è¯¢ä¸“ç”¨æ¨¡å‹ï¼ˆå¦‚æœè®¾ç½®ï¼Œè¯´æ˜è¿™æ˜¯ç”¨æˆ·æŸ¥è¯¢ï¼‰
    **kwargs
) -> str:
    """
    è‡ªå®šä¹‰ LLM å‡½æ•°ï¼Œæ ¹æ® llm_query_model æ˜¯å¦å­˜åœ¨ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
    
    Args:
        llm_query_model: ç”¨æˆ·æŸ¥è¯¢æ—¶ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
    """
    # å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹
    if llm_query_model:
        model = llm_query_model
        print(f"[ç”¨æˆ·æŸ¥è¯¢] ä½¿ç”¨æŸ¥è¯¢æ¨¡å‹: {model}")
    else:
        model = os.getenv("LLM_MODEL", "gpt-4o-mini")
        print(f"[å†…éƒ¨è°ƒç”¨] ä½¿ç”¨é»˜è®¤æ¨¡å‹: {model}")
    
    print(f"[DEBUG] keyword_extraction={keyword_extraction}")
    
    # ç§»é™¤ä¸åº”è¯¥ä¼ é€’ç»™ openai_complete_if_cache çš„å‚æ•°
    kwargs_copy = kwargs.copy()
    kwargs_copy.pop("hashing_kv", None)  # ç§»é™¤ hashing_kvï¼Œå®ƒç”± LightRAG å†…éƒ¨ç®¡ç†
    
    result = await openai_complete_if_cache(
        model=model,
        prompt=prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        llm_query_model=llm_query_model,
        keyword_extraction=keyword_extraction,
        api_key=os.getenv("OPENAI_API_KEY") or os.getenv("LLM_BINDING_API_KEY"),
        base_url=os.getenv("LLM_BINDING_HOST", "https://api.openai.com/v1"),
        **kwargs_copy,
    )
    print(f"[DEBUG] LLM è¿”å›å†…å®¹å‰ 100 å­—: {str(result)[:100]}")
    return result


async def main():
    print("\n" + "="*80)
    print("LightRAG ç”¨æˆ·æŸ¥è¯¢æ¨¡å‹é…ç½®ç¤ºä¾‹")
    print("="*80)
    
    # éªŒè¯å¿…è¦çš„ç¯å¢ƒå˜é‡
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("LLM_BINDING_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® OPENAI_API_KEY æˆ– LLM_BINDING_API_KEY")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®æˆ–è®¾ç½®ç¯å¢ƒå˜é‡")
        return
    
    llm_model = os.getenv('LLM_MODEL', 'gpt-4o-mini')
    llm_query_model = os.getenv('LLM_QUERY_MODEL', None)
    
    print(f"âœ“ é»˜è®¤ LLM æ¨¡å‹ï¼ˆçŸ¥è¯†åº“æ„å»ºï¼‰: {llm_model}")
    print(f"âœ“ æŸ¥è¯¢ LLM æ¨¡å‹ï¼ˆç”¨æˆ·æŸ¥è¯¢ï¼‰: {llm_query_model if llm_query_model else llm_model + ' (æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹)'}")
    print(f"âœ“ LLM æœåŠ¡: {os.getenv('LLM_BINDING_HOST', 'https://api.openai.com/v1')}")
    print(f"âœ“ Embedding æ¨¡å‹: {os.getenv('EMBEDDING_MODEL', 'bge-m3:latest')}")
    print(f"âœ“ Embedding æœåŠ¡: {os.getenv('EMBEDDING_BINDING_HOST', 'http://localhost:11434')}")
    print(f"âœ“ å·¥ä½œç›®å½•: {WORKING_DIR}")
    
    # åˆå§‹åŒ– RAG
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        embedding_func=EmbeddingFunc(
            embedding_dim=int(os.getenv("EMBEDDING_DIM", "1024")),
            max_token_size=int(os.getenv("MAX_EMBED_TOKENS", "8192")),
            func=lambda texts: ollama_embed(
                texts,
                embed_model=os.getenv("EMBEDDING_MODEL", "bge-m3:latest"),
                host=os.getenv("EMBEDDING_BINDING_HOST", "http://localhost:11434"),
            ),
        ),
    )
    
    print("\n[1/2] åˆå§‹åŒ–å­˜å‚¨...")
    await rag.initialize_storages()
    print("[2/2] åˆå§‹åŒ–ç®¡é“çŠ¶æ€...")
    await initialize_pipeline_status()
    print("âœ“ åˆå§‹åŒ–å®Œæˆ\n")
    
    # åœºæ™¯ 1: æ’å…¥æ–‡æ¡£ï¼ˆå†…éƒ¨è°ƒç”¨ï¼Œä½¿ç”¨ LLM_MODELï¼‰
    print("="*80)
    print("ğŸ“ åœºæ™¯ 1: æ’å…¥æ–‡æ¡£ï¼ˆå†…éƒ¨è°ƒç”¨ - çŸ¥è¯†åº“æ„å»ºï¼‰")
    print("="*80)
    print("é¢„æœŸè¡Œä¸ºï¼š")
    print(f"  - ä½¿ç”¨é»˜è®¤æ¨¡å‹: {llm_model}")
    print("  - ç”¨äºçŸ¥è¯†æå–ã€å®ä½“è¯†åˆ«ç­‰åå°ä»»åŠ¡")
    print("  - å¯ä»¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹ä»¥é™ä½æˆæœ¬")
    print("-"*80)
    
    test_content = (
        "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œ"
        "è‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½æ‰èƒ½å®Œæˆçš„ä»»åŠ¡çš„æ™ºèƒ½ç³»ç»Ÿã€‚"
        "è¿™äº›ä»»åŠ¡åŒ…æ‹¬è§†è§‰æ„ŸçŸ¥ã€è¯­éŸ³è¯†åˆ«ã€å†³ç­–åˆ¶å®šå’Œè¯­è¨€ç¿»è¯‘ç­‰ã€‚"
        "æœºå™¨å­¦ä¹ ï¼ˆMachine Learning, MLï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒå­é¢†åŸŸï¼Œ"
        "å®ƒä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®å’Œç»éªŒä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜¾å¼ç¼–ç¨‹ã€‚"
        "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ"
        "ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚è¡¨ç¤ºã€‚"
    )
    
    print(f"\næ’å…¥æ–‡æ¡£å†…å®¹é¢„è§ˆï¼š\n{test_content[:100]}...\n")
    await rag.ainsert(test_content)
    print("âœ“ æ–‡æ¡£æ’å…¥å®Œæˆï¼ˆå·²è§¦å‘çŸ¥è¯†å›¾è°±æ„å»ºï¼‰\n")
    
    # åœºæ™¯ 2: ç”¨æˆ·æŸ¥è¯¢ï¼ˆAPI è°ƒç”¨ï¼Œä½¿ç”¨ LLM_QUERY_MODELï¼‰
    print("="*80)
    print("ğŸ” åœºæ™¯ 2: ç”¨æˆ·æŸ¥è¯¢ï¼ˆAPI è°ƒç”¨ - æ¨¡æ‹Ÿç”¨æˆ·è¯·æ±‚ï¼‰")
    print("="*80)
    print("é¢„æœŸè¡Œä¸ºï¼š")
    if llm_query_model:
        print(f"  - ä½¿ç”¨æŸ¥è¯¢ä¸“ç”¨æ¨¡å‹: {llm_query_model}")
        print("  - é€‚ç”¨äºéœ€è¦é«˜è´¨é‡å“åº”çš„ç”¨æˆ·æŸ¥è¯¢")
    else:
        print(f"  - ä½¿ç”¨é»˜è®¤æ¨¡å‹: {llm_model} (æœªé…ç½® LLM_QUERY_MODEL)")
    print("  - é€‚ç”¨äº API æ¥å£ã€Web åº”ç”¨ç­‰é¢å‘ç”¨æˆ·çš„æŸ¥è¯¢")
    print("-"*80)
    
    user_question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿå®ƒä¸æœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
    print(f"\nç”¨æˆ·é—®é¢˜ï¼š{user_question}\n")
    
    param_user = QueryParam(
        mode="hybrid", 
        llm_query_model=llm_query_model,  # è®¾ç½®æŸ¥è¯¢ä¸“ç”¨æ¨¡å‹ï¼ˆè®¾ç½®æ­¤å‚æ•°å³è¡¨ç¤ºè¿™æ˜¯ç”¨æˆ·æŸ¥è¯¢ï¼‰
        response_type="Multiple Paragraphs"
    )
    
    result_user = await rag.aquery(user_question, param=param_user)
    print(f"æŸ¥è¯¢ç»“æœï¼š\n{'-'*80}")
    print(f"{str(result_user)[:300]}...")
    print(f"{'-'*80}\n")
    
    # åœºæ™¯ 3: å†…éƒ¨æŸ¥è¯¢ï¼ˆä¸é€šè¿‡ APIï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼‰
    print("="*80)
    print("âš™ï¸  åœºæ™¯ 3: ç¨‹åºå†…éƒ¨æŸ¥è¯¢ï¼ˆé API è°ƒç”¨ï¼‰")
    print("="*80)
    print("é¢„æœŸè¡Œä¸ºï¼š")
    print(f"  - ä½¿ç”¨é»˜è®¤æ¨¡å‹: {llm_model}")
    print("  - é€‚ç”¨äºç¨‹åºå†…éƒ¨é€»è¾‘ã€æ‰¹å¤„ç†ä»»åŠ¡ç­‰")
    print("  - ä¸è®¾ç½® is_user_query å’Œ llm_query_model")
    print("-"*80)
    
    internal_question = "æ·±åº¦å­¦ä¹ çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"
    print(f"\nå†…éƒ¨æŸ¥è¯¢ï¼š{internal_question}\n")
    
    param_internal = QueryParam(
        mode="local",
        only_need_context=False
    )  # ä¸è®¾ç½® llm_query_modelï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
    
    result_internal = await rag.aquery(internal_question, param=param_internal)
    print(f"æŸ¥è¯¢ç»“æœï¼š\n{'-'*80}")
    print(f"{str(result_internal)[:300]}...")
    print(f"{'-'*80}\n")
    
    # æ€»ç»“
    print("="*80)
    print("âœ… ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("="*80)
    print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
    print("  1. âœ“ è®¾ç½® LLM_MODEL ç”¨äºçŸ¥è¯†åº“æ„å»ºï¼ˆå†…éƒ¨ä»»åŠ¡ï¼‰")
    print("  2. âœ“ è®¾ç½® LLM_QUERY_MODEL ç”¨äºç”¨æˆ·æŸ¥è¯¢ï¼ˆå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨ LLM_MODELï¼‰")
    print("  3. âœ“ QueryParam ä¸­è®¾ç½® llm_query_model å³è¡¨ç¤ºè¿™æ˜¯ç”¨æˆ·æŸ¥è¯¢ï¼Œå°†ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹")
    print("  4. âœ“ é€šè¿‡æ¨¡å‹åç§°åŒºåˆ†ä¸åŒåœºæ™¯ï¼Œåç«¯å¯æ ¹æ®æ¨¡å‹è·¯ç”±åˆ°ä¸åŒå®ä¾‹")
    
    print("\nğŸ¯ å®é™…åº”ç”¨åœºæ™¯:")
    print("  â€¢ æˆæœ¬ä¼˜åŒ–ï¼šç”¨æˆ·æŸ¥è¯¢ç”¨äº‘ç«¯é«˜æ€§èƒ½æ¨¡å‹ï¼Œå†…éƒ¨ä»»åŠ¡ç”¨æœ¬åœ°ç»æµæ¨¡å‹")
    print("  â€¢ æ€§èƒ½ä¼˜åŒ–ï¼šä¸åŒåœºæ™¯ä½¿ç”¨ä¸åŒä¼˜åŒ–çš„æ¨¡å‹")
    print("  â€¢ åç«¯è·¯ç”±ï¼šé€šè¿‡æ¨¡å‹åç§°è·¯ç”±åˆ°ä¸åŒçš„æœåŠ¡å®ä¾‹")
    print("  â€¢ çµæ´»éƒ¨ç½²ï¼šç›¸åŒ API åœ°å€ï¼Œä¸åŒæ¨¡å‹åç§°å¯¹åº”ä¸åŒåç«¯")
    
    print("\nğŸ“ é…ç½®ç¤ºä¾‹ï¼ˆ.env æ–‡ä»¶ï¼‰:")
    print("  LLM_MODEL=qwen-local              # æœ¬åœ°æ¨¡å‹ç”¨äºçŸ¥è¯†åº“æ„å»º")
    print("  LLM_QUERY_MODEL=gpt-4o            # äº‘ç«¯æ¨¡å‹ç”¨äºç”¨æˆ·æŸ¥è¯¢")
    print("  LLM_BINDING_HOST=http://localhost:8000/v1")
    print("  # åç«¯æœåŠ¡å™¨æ ¹æ®æ¨¡å‹åç§°è·¯ç”±ï¼š")
    print("  #   - qwen-local -> æœ¬åœ° Qwen å®ä¾‹")
    print("  #   - gpt-4o -> äº‘ç«¯ OpenAI å®ä¾‹")
    
    print("\nğŸ“š ç›¸å…³èµ„æº:")
    print("  â€¢ API æœåŠ¡å™¨: lightrag/api/lightrag_server.py")
    print("  â€¢ æŸ¥è¯¢è·¯ç”±: lightrag/api/routers/query.py")
    print("  â€¢ æ ¸å¿ƒå®ç°: lightrag/llm/openai.py")
    print()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ æç¤ºï¼š")
        print("  1. æ£€æŸ¥ .env é…ç½®æ˜¯å¦æ­£ç¡®")
        print("  2. ç¡®è®¤ Embedding æœåŠ¡ï¼ˆå¦‚ Ollamaï¼‰æ˜¯å¦æ­£åœ¨è¿è¡Œ")
        print("  3. ç¡®è®¤ LLM API å¯†é’¥å’ŒæœåŠ¡åœ°å€æ˜¯å¦æ­£ç¡®")
