"""
ç¤ºä¾‹ï¼šæ ¹æ® is_user_query æ ‡å¿—æ·»åŠ ä¸åŒçš„ HTTP Header

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ç”¨æˆ·æŸ¥è¯¢å’Œç³»ç»Ÿå†…éƒ¨è°ƒç”¨æ—¶ä½¿ç”¨ä¸åŒçš„ HTTP Headerã€‚
- ç”¨æˆ·æŸ¥è¯¢ï¼ˆAPI è°ƒç”¨ï¼‰ï¼šæ·»åŠ  "X-User-Query: true" Header
- å†…éƒ¨è°ƒç”¨ï¼ˆçŸ¥è¯†åº“æ„å»ºï¼‰ï¼šæ·»åŠ  "X-User-Query: false" Header

ä½¿ç”¨åœºæ™¯ï¼š
1. ç›¸åŒçš„å¤§æ¨¡å‹æœåŠ¡å™¨åœ°å€ï¼Œä½†éœ€è¦æ ¹æ®è°ƒç”¨ç±»å‹åŒºåˆ†è¯·æ±‚
2. åç«¯æœåŠ¡å™¨æ ¹æ® Header è·¯ç”±åˆ°ä¸åŒçš„æ¨¡å‹å®ä¾‹ï¼ˆå¦‚é˜¿é‡Œäº‘ Qwen vs æœ¬åœ° Qwenï¼‰
3. è®¡è´¹æˆ–ç›‘æ§ï¼šåŒºåˆ†ç”¨æˆ·æŸ¥è¯¢å’Œç³»ç»Ÿå†…éƒ¨è°ƒç”¨çš„èµ„æºä½¿ç”¨

è¿è¡Œè¦æ±‚ï¼š
- éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆé€šè¿‡ .env æ–‡ä»¶æˆ–ç›´æ¥è®¾ç½®ï¼‰
- OPENAI_API_KEY æˆ– LLM_BINDING_API_KEYï¼šLLM API å¯†é’¥
- LLM_BINDING_HOSTï¼šLLM æœåŠ¡åœ°å€ï¼ˆé»˜è®¤ï¼šhttps://api.openai.com/v1ï¼‰
- LLM_MODELï¼šLLM æ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼šgpt-4o-miniï¼‰
- EMBEDDING_BINDING_HOSTï¼šEmbedding æœåŠ¡åœ°å€ï¼ˆé»˜è®¤ï¼šhttp://localhost:11434ï¼‰
- EMBEDDING_MODELï¼šEmbedding æ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼šbge-m3:latestï¼‰
"""

import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import openai_complete_if_cache
from lightrag.llm.ollama import ollama_embed
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status
from dotenv import load_dotenv

load_dotenv(dotenv_path=".env", override=False)

WORKING_DIR = "./demoDir"


async def llm_model_func_with_header(
    prompt, 
    system_prompt=None, 
    history_messages=[], 
    keyword_extraction=False,
    is_user_query=False,  # å…³é”®å‚æ•°ï¼šåŒºåˆ†ç”¨æˆ·æŸ¥è¯¢å’Œå†…éƒ¨è°ƒç”¨
    **kwargs
) -> str:
    """
    è‡ªå®šä¹‰ LLM å‡½æ•°ï¼Œæ ¹æ® is_user_query æ ‡å¿—ä½¿ç”¨ä¸åŒçš„ API KEY
    
    Args:
        is_user_query: True è¡¨ç¤ºç”¨æˆ·æŸ¥è¯¢ï¼ˆAPI è°ƒç”¨ï¼‰ï¼ŒFalse è¡¨ç¤ºå†…éƒ¨è°ƒç”¨
    """
    # Debug: æ‰“å° is_user_query å’Œ OPENAI_QUERY_KEY
    print(f"[DEBUG] is_user_query={is_user_query}, keyword_extraction={keyword_extraction}")
    print(f"[DEBUG] OPENAI_QUERY_KEY={os.getenv('OPENAI_QUERY_KEY')}")
    print(f"[DEBUG] OPENAI_API_KEY={os.getenv('OPENAI_API_KEY')}")
    
    # ç§»é™¤ä¸åº”è¯¥ä¼ é€’ç»™ openai_complete_if_cache çš„å‚æ•°
    kwargs_copy = kwargs.copy()
    kwargs_copy.pop("hashing_kv", None)  # ç§»é™¤ hashing_kvï¼Œå®ƒç”± LightRAG å†…éƒ¨ç®¡ç†
    
    result = await openai_complete_if_cache(
        model=os.getenv("LLM_MODEL", "gpt-4o-mini"),
        prompt=prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        is_user_query=is_user_query,
        keyword_extraction=keyword_extraction,
        api_key=os.getenv("OPENAI_API_KEY") or os.getenv("LLM_BINDING_API_KEY"),
        base_url=os.getenv("LLM_BINDING_HOST", "https://api.openai.com/v1"),
        **kwargs_copy,
    )
    print(f"[DEBUG] LLM è¿”å›å†…å®¹å‰ 100 å­—: {str(result)[:100]}")
    return result


async def main():
    print("\n" + "="*80)
    print("LightRAG ç”¨æˆ·æŸ¥è¯¢ Header é…ç½®ç¤ºä¾‹")
    print("="*80)
    
    # éªŒè¯å¿…è¦çš„ç¯å¢ƒå˜é‡
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("LLM_BINDING_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® OPENAI_API_KEY æˆ– LLM_BINDING_API_KEY")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®æˆ–è®¾ç½®ç¯å¢ƒå˜é‡")
        return
    
    print(f"âœ“ LLM æ¨¡å‹: {os.getenv('LLM_MODEL', 'gpt-4o-mini')}")
    print(f"âœ“ LLM æœåŠ¡: {os.getenv('LLM_BINDING_HOST', 'https://api.openai.com/v1')}")
    print(f"âœ“ Embedding æ¨¡å‹: {os.getenv('EMBEDDING_MODEL', 'bge-m3:latest')}")
    print(f"âœ“ Embedding æœåŠ¡: {os.getenv('EMBEDDING_BINDING_HOST', 'http://localhost:11434')}")
    print(f"âœ“ å·¥ä½œç›®å½•: {WORKING_DIR}")
    print(f"[DEBUG] å½“å‰ç¯å¢ƒ OPENAI_QUERY_KEY={os.getenv('OPENAI_QUERY_KEY')}")
    print(f"[DEBUG] å½“å‰ç¯å¢ƒ OPENAI_API_KEY={os.getenv('OPENAI_API_KEY')}")
    
    # åˆå§‹åŒ– RAG
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func_with_header,
        embedding_func=EmbeddingFunc(
            embedding_dim=int(os.getenv("EMBEDDING_DIM", "1024")),
            max_token_size=int(os.getenv("MAX_EMBED_TOKENS", "8192")),
            func=lambda texts: ollama_embed(
                texts,
                embed_model=os.getenv("EMBEDDING_MODEL", "bge-m3:latest"),
                host=os.getenv("EMBEDDING_BINDING_HOST", "http://localhost:11434"),
            ),
        ),
    )
    
    print("\n[1/2] åˆå§‹åŒ–å­˜å‚¨...")
    await rag.initialize_storages()
    print("[2/2] åˆå§‹åŒ–ç®¡é“çŠ¶æ€...")
    await initialize_pipeline_status()
    print("âœ“ åˆå§‹åŒ–å®Œæˆ\n")
    
    # åœºæ™¯ 1: æ’å…¥æ–‡æ¡£ï¼ˆå†…éƒ¨è°ƒç”¨ï¼Œis_user_query=Falseï¼‰
    print("="*80)
    print("ğŸ“ åœºæ™¯ 1: æ’å…¥æ–‡æ¡£ï¼ˆå†…éƒ¨è°ƒç”¨ - çŸ¥è¯†åº“æ„å»ºï¼‰")
    print("="*80)
    print("é¢„æœŸè¡Œä¸ºï¼š")
    print("  - LLM è°ƒç”¨æ—¶æ·»åŠ  'X-User-Query: false' Header")
    print("  - åç«¯å¯æ ¹æ®æ­¤ Header è·¯ç”±åˆ°å†…éƒ¨å¤„ç†æ¨¡å‹")
    print("  - é€‚ç”¨äºçŸ¥è¯†æå–ã€å®ä½“è¯†åˆ«ç­‰åå°ä»»åŠ¡")
    print("-"*80)
    
    test_content = (
        "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œ"
        "è‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½æ‰èƒ½å®Œæˆçš„ä»»åŠ¡çš„æ™ºèƒ½ç³»ç»Ÿã€‚"
        "è¿™äº›ä»»åŠ¡åŒ…æ‹¬è§†è§‰æ„ŸçŸ¥ã€è¯­éŸ³è¯†åˆ«ã€å†³ç­–åˆ¶å®šå’Œè¯­è¨€ç¿»è¯‘ç­‰ã€‚"
        "æœºå™¨å­¦ä¹ ï¼ˆMachine Learning, MLï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒå­é¢†åŸŸï¼Œ"
        "å®ƒä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®å’Œç»éªŒä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜¾å¼ç¼–ç¨‹ã€‚"
        "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ"
        "ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚è¡¨ç¤ºã€‚"
    )
    
    print(f"\næ’å…¥æ–‡æ¡£å†…å®¹é¢„è§ˆï¼š\n{test_content[:100]}...\n")
    await rag.ainsert(test_content)
    print("âœ“ æ–‡æ¡£æ’å…¥å®Œæˆï¼ˆå·²è§¦å‘çŸ¥è¯†å›¾è°±æ„å»ºï¼‰\n")
    
    # åœºæ™¯ 2: ç”¨æˆ·æŸ¥è¯¢ï¼ˆAPI è°ƒç”¨ï¼Œis_user_query=Trueï¼‰
    print("="*80)
    print("ğŸ” åœºæ™¯ 2: ç”¨æˆ·æŸ¥è¯¢ï¼ˆAPI è°ƒç”¨ - æ¨¡æ‹Ÿç”¨æˆ·è¯·æ±‚ï¼‰")
    print("="*80)
    print("é¢„æœŸè¡Œä¸ºï¼š")
    print("  - LLM è°ƒç”¨æ—¶æ·»åŠ  'X-User-Query: true' Header")
    print("  - åç«¯å¯æ ¹æ®æ­¤ Header è·¯ç”±åˆ°ç”¨æˆ·æœåŠ¡æ¨¡å‹")
    print("  - é€‚ç”¨äº API æ¥å£ã€Web åº”ç”¨ç­‰é¢å‘ç”¨æˆ·çš„æŸ¥è¯¢")
    print("-"*80)
    
    user_question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿå®ƒä¸æœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
    print(f"\nç”¨æˆ·é—®é¢˜ï¼š{user_question}\n")
    
    param_user = QueryParam(
        mode="hybrid", 
        is_user_query=True,  # æ˜¾å¼æ ‡è®°ä¸ºç”¨æˆ·æŸ¥è¯¢
        response_type="Multiple Paragraphs"
    )
    
    result_user = await rag.aquery(user_question, param=param_user)
    print(f"æŸ¥è¯¢ç»“æœï¼š\n{'-'*80}")
    print(f"{str(result_user)[:300]}...")
    print(f"{'-'*80}\n")
    
    # åœºæ™¯ 3: å†…éƒ¨æŸ¥è¯¢ï¼ˆä¸é€šè¿‡ APIï¼Œis_user_query=Falseï¼‰
    print("="*80)
    print("âš™ï¸  åœºæ™¯ 3: ç¨‹åºå†…éƒ¨æŸ¥è¯¢ï¼ˆé API è°ƒç”¨ï¼‰")
    print("="*80)
    print("é¢„æœŸè¡Œä¸ºï¼š")
    print("  - LLM è°ƒç”¨æ—¶æ·»åŠ  'X-User-Query: false' Header")
    print("  - åç«¯å¯æ ¹æ®æ­¤ Header è·¯ç”±åˆ°å†…éƒ¨å¤„ç†æ¨¡å‹")
    print("  - é€‚ç”¨äºç¨‹åºå†…éƒ¨é€»è¾‘ã€æ‰¹å¤„ç†ä»»åŠ¡ç­‰")
    print("-"*80)
    
    internal_question = "æ·±åº¦å­¦ä¹ çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"
    print(f"\nå†…éƒ¨æŸ¥è¯¢ï¼š{internal_question}\n")
    
    param_internal = QueryParam(
        mode="local",
        only_need_context=False
    )  # ä¸è®¾ç½® is_user_queryï¼Œé»˜è®¤ä¸º False
    
    result_internal = await rag.aquery(internal_question, param=param_internal)
    print(f"æŸ¥è¯¢ç»“æœï¼š\n{'-'*80}")
    print(f"{str(result_internal)[:300]}...")
    print(f"{'-'*80}\n")
    
    # æ€»ç»“
    print("="*80)
    print("âœ… ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("="*80)
    print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
    print("  1. âœ“ é€šè¿‡ API è·¯ç”±ï¼ˆ/query, /query/streamï¼‰çš„æŸ¥è¯¢ä¼šè‡ªåŠ¨è®¾ç½® is_user_query=True")
    print("  2. âœ“ çŸ¥è¯†åº“æ„å»ºï¼ˆæ’å…¥æ–‡æ¡£ã€å®ä½“æå–ç­‰ï¼‰è‡ªåŠ¨ä½¿ç”¨ is_user_query=False")
    print("  3. âœ“ ç¨‹åºå†…éƒ¨ç›´æ¥è°ƒç”¨ aquery æ—¶ï¼Œé»˜è®¤ is_user_query=False")
    print("  4. âœ“ å¯ä»¥æ ¹æ® Header åœ¨åç«¯æœåŠ¡å™¨è¿›è¡Œè·¯ç”±ã€è®¡è´¹æˆ–ç›‘æ§")
    
    print("\nğŸ¯ å®é™…åº”ç”¨åœºæ™¯:")
    print("  â€¢ ä½¿ç”¨ä¸åŒçš„æ¨¡å‹å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆé«˜æ€§èƒ½ï¼‰å’Œå†…éƒ¨ä»»åŠ¡ï¼ˆç»æµå‹ï¼‰")
    print("  â€¢ åŸºäºè¯·æ±‚ç±»å‹çš„è®¡è´¹å’Œé…é¢ç®¡ç†")
    print("  â€¢ è¯·æ±‚ä¼˜å…ˆçº§æ§åˆ¶ï¼ˆç”¨æˆ·æŸ¥è¯¢ä¼˜å…ˆï¼‰")
    print("  â€¢ ç›‘æ§å’Œæ—¥å¿—åˆ†æï¼ˆåŒºåˆ†ç”¨æˆ·è¡Œä¸ºå’Œç³»ç»Ÿè¡Œä¸ºï¼‰")
    
    print("\nï¿½ ç›¸å…³èµ„æº:")
    print("  â€¢ å®Œæ•´æ–‡æ¡£: docs/UserQueryHeaderConfiguration.md")
    print("  â€¢ API æœåŠ¡å™¨: lightrag/api/lightrag_server.py")
    print("  â€¢ æŸ¥è¯¢è·¯ç”±: lightrag/api/routers/query.py")
    print()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ æç¤ºï¼š")
        print("  1. æ£€æŸ¥ .env é…ç½®æ˜¯å¦æ­£ç¡®")
        print("  2. ç¡®è®¤ Embedding æœåŠ¡ï¼ˆå¦‚ Ollamaï¼‰æ˜¯å¦æ­£åœ¨è¿è¡Œ")
        print("  3. ç¡®è®¤ LLM API å¯†é’¥å’ŒæœåŠ¡åœ°å€æ˜¯å¦æ­£ç¡®")
